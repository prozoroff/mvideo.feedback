{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pymorphy2\n",
    "import re\n",
    "from stop_words import get_stop_words\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data frame size: (15587, 5)\n"
     ]
    }
   ],
   "source": [
    "fields = ['sku', 'comment', 'commentNegative', 'commentPositive', 'reting']\n",
    "raw_data = pd.read_csv('feedback.csv/X_train.csv', delimiter=',', usecols=fields)\n",
    "print('Data frame size:',raw_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feedbacks by rating, test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "feedbacks_keys = ['1', '2', '3', '4', '5', '123', '1234', '12345', '23'];\n",
    "feedbacks = {}\n",
    "rows = list(map(lambda x: x[1], raw_data.iterrows()))\n",
    "np.random.shuffle(rows)\n",
    "train_size = int(raw_data.shape[0]*.95)\n",
    "rows_all = rows[:train_size]\n",
    "rows_test = rows[train_size:]\n",
    "\n",
    "for row in rows_all:\n",
    "    if not row['reting'].is_integer():\n",
    "        continue\n",
    "    rating = row['reting']\n",
    "    for key in feedbacks_keys:\n",
    "        if str(int(rating)) in key:\n",
    "            if not key in feedbacks:\n",
    "                feedbacks[key] = []\n",
    "            feedbacks[key].append(row)\n",
    "\n",
    "for f in feedbacks:\n",
    "    np.random.shuffle(feedbacks[f])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to normilize text and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "regex_clear = re.compile('[^а-яё]',re.DOTALL)\n",
    "regex_space = re.compile('\\s{1,}',re.DOTALL)\n",
    "\n",
    "def clear(text):\n",
    "    tmp = re.sub(regex_clear, ' ', text.lower().replace('ё', 'е'))\n",
    "    return re.sub(regex_space, ' ', tmp)\n",
    "\n",
    "def prepare(text):\n",
    "    return text.lower()\n",
    "\n",
    "def to_normal(word):\n",
    "    return morph.parse(word)[0].normal_form\n",
    "    \n",
    "def normalize(text):\n",
    "    words = []\n",
    "    for word in text.split(' '):\n",
    "        words.append(to_normal(word))\n",
    "    norm=' '.join(words)  \n",
    "    return norm.replace('ё', 'е')\n",
    "\n",
    "class TextNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        cleared = [clear(text) for text in texts]\n",
    "        return [normalize(text) for text in texts]\n",
    "    \n",
    "class TextCleaner(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, texts):\n",
    "        return [prepare(text) for text in texts]\n",
    "    \n",
    "def evaluate_cross_validation(clf, X, y, K):\n",
    "    cv = StratifiedKFold(K, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(clf, X, y, scoring='accuracy', cv=cv) \n",
    "    print(scores)\n",
    "    print(\"Mean score: {0:.3f} (+/-{1:.3f})\".format(scores.mean(), scores.std()))\n",
    "    \n",
    "class RandomForestClassifierWithCoef(RandomForestClassifier):\n",
    "    def fit(self, *args, **kwargs):\n",
    "        super(RandomForestClassifierWithCoef, self).fit(*args, **kwargs)\n",
    "        self.coef_ = self.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feature union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('union', FeatureUnion(\n",
    "        transformer_list=[\n",
    "            ('count_vectorizer', Pipeline([\n",
    "                ('selector', TextCleaner()),\n",
    "                ('cv', CountVectorizer(min_df=0.0001, max_df=0.95, ngram_range=(4,4), analyzer=\"char_wb\")),\n",
    "                #('best', TruncatedSVD(n_components=100)),\n",
    "            ])),\n",
    "            ('tfidf', Pipeline([\n",
    "                 ('selector', TextCleaner()),\n",
    "                 ('tfidf', TfidfVectorizer(analyzer=\"word\", stop_words=get_stop_words('ru'))),\n",
    "                 #('best', TruncatedSVD(n_components=10)), \n",
    "             ])),\n",
    "            ('count_vectorizer_default', Pipeline([\n",
    "                ('selector', TextNormalizer()),\n",
    "                ('cv', CountVectorizer(min_df=0.0001, max_df=0.95, stop_words=get_stop_words('ru'), analyzer='word')),                                \n",
    "            ])),\n",
    "\n",
    "        ],\n",
    "        transformer_weights={ \n",
    "            'count_vectorizer': 1,\n",
    "            'tfidf': 1,\n",
    "            'count_vectorizer_default': 1\n",
    "       },\n",
    "    )),\n",
    "\n",
    "    #('model', SVC(kernel='linear')) #rbf, sigmoid, polynomial\n",
    "    ('model', RandomForestClassifierWithCoef(n_estimators=512, min_samples_leaf=1, n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normilized and shuffled datasets to fit classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getShuffledNormalizedFeedbacks(keys):\n",
    "    result = []\n",
    "    feedbacks_by_keys = []\n",
    "    feedbacks_limit = 0\n",
    "    for key in keys:\n",
    "        f = feedbacks[key]\n",
    "        feedbacks_limit = len(f) if feedbacks_limit == 0 else min(feedbacks_limit, len(f))\n",
    "        feedbacks_by_keys.append(f)\n",
    "    for f in feedbacks_by_keys:\n",
    "        result.extend(list(f[:feedbacks_limit]))\n",
    "    np.random.shuffle(result)\n",
    "    return result\n",
    "\n",
    "def toInput(num, val):\n",
    "    if num == val:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "feedbacks_5 = getShuffledNormalizedFeedbacks(['5', '1234']);\n",
    "feedbacks_4 = getShuffledNormalizedFeedbacks(['4', '123']);\n",
    "feedbacks_1 = getShuffledNormalizedFeedbacks(['1', '23']);\n",
    "feedbacks_2 = getShuffledNormalizedFeedbacks(['2', '3']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_dataframe(rows, num):\n",
    "    tmp_df = pd.DataFrame()\n",
    "    tmp_df['comment'] = list(r['comment'] for r in rows)\n",
    "    if num > 0:\n",
    "        tmp_df['result'] = list(toInput(r['reting'],num) for r in rows)\n",
    "    else:\n",
    "        tmp_df['result'] = list(int(r['reting']) for r in rows)\n",
    "    return tmp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit classifiers\n",
    "\n",
    "#### five or below classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 against 1234 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.78      0.78      0.78       306\n",
      "          1       0.77      0.77      0.77       282\n",
      "\n",
      "avg / total       0.78      0.78      0.78       588\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedback_5_df = to_dataframe(feedbacks_5, 5)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feedback_5_df['comment'], feedback_5_df['result'], test_size = 0.05)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('5 against 1234 report:')\n",
    "print(classification_report(pipeline.predict(X_test), y_test))\n",
    "pickle.dump(pipeline, open('mvideo.feedback.pickle.5', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### four or below classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 against 123 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.65      0.72      0.69       112\n",
      "          1       0.75      0.69      0.72       138\n",
      "\n",
      "avg / total       0.71      0.70      0.70       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedback_4_df = to_dataframe(feedbacks_4, 4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feedback_4_df['comment'], feedback_4_df['result'], test_size = 0.05)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('4 against 123 report:')\n",
    "print(classification_report(pipeline.predict(X_test), y_test))\n",
    "pickle.dump(pipeline, open('mvideo.feedback.pickle.4', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### one or above classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 against 23 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.79      0.59      0.68        83\n",
      "          1       0.56      0.77      0.65        57\n",
      "\n",
      "avg / total       0.70      0.66      0.67       140\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedbacks_1_df = to_dataframe(feedbacks_1, 1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feedbacks_1_df['comment'], feedbacks_1_df['result'], test_size = 0.05)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('1 against 23 report:')\n",
    "print(classification_report(pipeline.predict(X_test), y_test))\n",
    "pickle.dump(pipeline, open('mvideo.feedback.pickle.1', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### two or three classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 against 3 report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "         -1       0.64      0.61      0.63        44\n",
      "          1       0.57      0.61      0.59        38\n",
      "\n",
      "avg / total       0.61      0.61      0.61        82\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedbacks_2_df = to_dataframe(feedbacks_2, 2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(feedbacks_2_df['comment'], feedbacks_2_df['result'], test_size = 0.05)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('2 against 3 report:')\n",
    "print(classification_report(pipeline.predict(X_test), y_test))\n",
    "pickle.dump(pipeline, open('mvideo.feedback.pickle.2', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### feedback prediction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_5 = pickle.load(open('mvideo.feedback.pickle.5', 'rb'))\n",
    "pipeline_4 = pickle.load(open('mvideo.feedback.pickle.4', 'rb'))\n",
    "pipeline_2 = pickle.load(open('mvideo.feedback.pickle.2', 'rb'))\n",
    "pipeline_1 = pickle.load(open('mvideo.feedback.pickle.1', 'rb'))\n",
    "\n",
    "def predict(text):\n",
    "    if pipeline_5.predict([text])[0] == 1:\n",
    "        return 5\n",
    "    elif pipeline_4.predict([text])[0] == 1:\n",
    "        return 4\n",
    "    elif pipeline_1.predict([text])[0] == 1:\n",
    "        return 1\n",
    "    elif pipeline_2.predict([text])[0] == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "def predict_all(text_array):\n",
    "    result = []\n",
    "    for text in text_array:\n",
    "        result.append(predict(text))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Стильный,хорошо смотрится ... actual: 1.0 predicted: 1\n",
      "Компьютер очень качественный не считая того, что н ... actual: 5.0 predicted: 5\n",
      "Класс!!!!!  Можно камеру подключить к  ps3 ... actual: 5.0 predicted: 5\n",
      "Брали в М.Видео месяц назад и пока не пожалели. От ... actual: 5.0 predicted: 5\n",
      "Рекомендую посмотреть всем мужчинам, несмотря на т ... actual: 5.0 predicted: 5\n",
      "Телефончик подарил муж.Спасибо тебе милый!Я в вост ... actual: 5.0 predicted: 5\n",
      "Самый Coolьный телефон по всей планете! ... actual: 5.0 predicted: 5\n",
      "Отлично работает, очень приятный дизайн. ... actual: 5.0 predicted: 5\n",
      "Телевизор за свои деньги устроил полностью, всё чт ... actual: 4.0 predicted: 4\n",
      "Очень классный плеер, всё в нём круто кроме батаре ... actual: 5.0 predicted: 5\n",
      "Симпатичный дизайн, хорошо вписалась в ванную. Оче ... actual: 5.0 predicted: 5\n",
      "Этот тостер нам с супругой подарила теща. Поначалу ... actual: 3.0 predicted: 3\n",
      "Использование около года, вкус воды лучше чем в пл ... actual: 5.0 predicted: 5\n",
      "Я пользуюсь уже больше года, за этот период не был ... actual: 5.0 predicted: 5\n",
      "Компактная, удобная плитка. Действительно, быстро  ... actual: 5.0 predicted: 5\n",
      "Кофемолка мне досталась со скидкой, почти, в полов ... actual: 5.0 predicted: 5\n",
      "Цена. Большой стакан для помола. Металлический кор ... actual: 4.7 predicted: 1\n",
      "Чесно говоря не увлекла. Поиграл 2 часа не мог нар ... actual: 1.0 predicted: 4\n",
      "Плюсы:   -красиво выглядит  Минусы  -бракованный   ... actual: 1.0 predicted: 1\n",
      "Первое время был неприятный запах пластика, что не ... actual: 5.0 predicted: 5\n"
     ]
    }
   ],
   "source": [
    "for row in rows_all[:20]:\n",
    "    print(row['comment'][:50],'... actual:',row['reting'],'predicted:',predict(row['comment']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### results on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          1       0.67      0.49      0.56       102\n",
      "          2       0.20      0.19      0.19        43\n",
      "          3       0.15      0.27      0.19        41\n",
      "          4       0.46      0.31      0.37       178\n",
      "          5       0.74      0.84      0.78       416\n",
      "\n",
      "avg / total       0.61      0.61      0.60       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feedbacks_test = to_dataframe(rows_test, 0)\n",
    "print(classification_report(predict_all(feedbacks_test['comment']), feedbacks_test['result']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
